{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries \n",
    "\n",
    "# Pandas and numpy for data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Seaborn / matplotlib for visualization \n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "# Import the trees from sklearn\n",
    "from sklearn import tree\n",
    "\n",
    "# Helper function to split our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Helper fuctions to evaluate our model.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report, roc_auc_score, f1_score\n",
    "\n",
    "\n",
    "# Helper function for hyper-parameter turning.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Import our Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import plot_tree, export_text \n",
    "# Import our Random Forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use inline so our visualizations display in notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Steps when building a Machine Learning Model. \n",
    "1. Inspect and explore data.\n",
    "2. Select and engineer features.\n",
    "3. Build and train model.\n",
    "4. Evaluate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1 Inspect and explore data.\n",
    "* Load titanic data\n",
    "* Visualize all the data using sns.pairplot\n",
    "* Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the titanic data set.\n",
    "df= pd.read_csv('data/titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all the data using sns.pairplot\n",
    "sns.pairplot(df, hue= 'survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2 Select and engineer features.\n",
    "1. Fill age null values with -999\n",
    "1. Convert to numerical values if need be by using `pd.get_dummies()`\n",
    "1. Create a list of the features you are going to use.  In this case use as many or as little as you would like.\n",
    "1. Define our `X` and `y`\n",
    "1. Split our data into trainig and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill age null values with -999\n",
    "df['age'] = df['age'].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert to numerical values if need be by using `pd.get_dummies()`\n",
    "df = pd.get_dummies(df, columns = ['sex', 'pclass','embarked'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a list of the features we are going to use.\n",
    "feature= ['fare', 'age', 'sex_male', 'fare', 'pclass_2', 'pclass_3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our `X` and `y`\n",
    "X = df[feature]\n",
    "y = df['survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into trainig and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=45)\n",
    "print('Lenght of our Training data:', X_train.shape[0], '\\nLength of our Testing data:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3 Build and train model.\n",
    "1. For our first pass, initialize our model with `max_depth=2`.\n",
    "2. Fit our model with our training data. \n",
    "3. Make predictions of our testing data. \n",
    "4. Evaluate and print our model scores using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "    * To calculate auc score you have to get the predicted probabilites for the Survived class using `model.predict_proba(X_test)[:,1]`\n",
    "5. Visualize our Decision Tree using provided code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our first pass, initialize our model with `max_depth=2`.\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model with our training data. \n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions of our testing data. \n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate and print our model scores using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\n",
    "print(\"Accuracy Score: %f\" % accuracy)\n",
    "\n",
    "precision = precision_score(y_true = y_test, y_pred = y_pred)\n",
    "print(\"Precision Score: %f\" % precision)\n",
    "\n",
    "recall = recall_score(y_true = y_test,  y_pred = y_pred)\n",
    "print(\"Recall Score: %f\" % recall)\n",
    "\n",
    "f1 = f1_score(y_true = y_test,  y_pred = y_pred)\n",
    "print('F1 Score: %f' % f1)\n",
    "\n",
    "# Calculate predicted probabilities\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Keep only the proba for True\n",
    "y_pred_proba = y_pred_proba[:,1]\n",
    "\n",
    "# Compute auc score\n",
    "auc = roc_auc_score(y_true = y_test, y_score = y_pred_proba)\n",
    "print('AUC Score: %f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize your tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names= ['died', 'survived']\n",
    "plot_tree(model, feature_names = feature, class_names = class_names, filled= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking the right parameters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning of your Decision Tree using GridSearch or RandomizedSearch\n",
    "\n",
    "### For assistance on this, look at Steves TA Tips code in `TA-Tips/random_forest_tuning.ipynb`\n",
    "\n",
    "\n",
    "1. Make a dictionary of at least 3 parameters and a list of 3 values for each for your grid search. \n",
    "1. Initalize your GridSearchCV with a DecisionTreeClassifier, your param_grid, and what you are optimizing for.  Choose any of the five optimization strategies; accuracy, precision, recall, f1, or roc_auc.\n",
    "1. Fit your GridSearchCV with your training data. \n",
    "1. Print the parameters of your best model. \n",
    "1. Evaluate your best model using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "1. Visualize your best tree.\n",
    "1. Which feature was your most important feature?\n",
    "\n",
    "```python\n",
    "tree.DecisionTreeClassifier(\n",
    "    *,\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    class_weight=None,\n",
    "    presort='deprecated',\n",
    "    ccp_alpha=0.0,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tips on how to customize / set the paramters in the decision tree.](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. Make a dictionary of at least 3 parameters and a list of 3 values for each for your grid search.from sklearn.model_selection import GridSearchCV\n",
    "params = { \n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'splitter' : ['best'],\n",
    "    'max_features': ['sqrt', 'log2', None], \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initalize your GridSearchCV with a DecisionTreeClassifier, your param_grid, and what you are optimizing for.  Choose any of the five optimization strategies; accuracy, precision, recall, f1, or roc_auc.\n",
    "grid_search_cv =  GridSearchCV(model, param_grid=params, scoring = 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit your GridSearchCV with your training data. \n",
    "grid_search_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the parameters of your best model. \n",
    "# Print the best parameters it found\n",
    "print( grid_search_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate your best model using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "\n",
    "# This command gives you the best tree\n",
    "model = grid_search_cv.best_estimator_\n",
    "\n",
    "# Now lets evaluate our model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_test,y_pred=y_pred)\n",
    "print(\"Accuracy Score: %f\" % accuracy)\n",
    "\n",
    "precision = precision_score(y_true=y_test,y_pred=y_pred)\n",
    "print(\"Precision Score: %f\" % precision)\n",
    "\n",
    "recall = recall_score(y_true=y_test,y_pred=y_pred)\n",
    "print(\"Recall Score: %f\" % recall)\n",
    "\n",
    "f1 = f1_score(y_true=y_test,y_pred=y_pred)\n",
    "print('F1 Score: %f' % f1)\n",
    "\n",
    "# Calculate predicted probabilities, keep only probability for when class = 1\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "auc = roc_auc_score(y_true = y_test, y_score = y_pred_proba)\n",
    "print('AUC Score: %f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Which feature was your most important feature?\n",
    "# Now lets look at our feature importances\n",
    "feature_imp_titanic = pd.DataFrame.from_dict( {'feature_importance': model.feature_importances_,\n",
    "                                       'feature':feature}).sort_values('feature_importance', ascending=False)\n",
    "feature_imp_titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now onto Random Forests...\n",
    "Were going to do the same with, but this time with a random forest. Remeber... Repetition is the father of learning.\n",
    "\n",
    "1. Make a dictionary of at least 3 parameters and a list of 3 values for each for your grid search. \n",
    "1. Initalize your GridSearchCV with a RandomForestClassifer, your param_grid, and what you are optimizing for.  Choose any of the five optimization strategies; accuracy, precision, recall, f1, or roc_auc.\n",
    "1. Fit your GridSearchCV with your training data. \n",
    "1. Print the parameters of your best model. \n",
    "1. Evaluate your best model using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "1. Which feature was your most important feature?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters of the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    *,\n",
    "    criterion='gini',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=None,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0,\n",
    "    max_samples=None,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make a dictionary of at least 3 parameters and a list of 3 values for each for your grid search. \n",
    "params_rf = {\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth': [2, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2', None], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initalize your GridSearchCV or RandomizedSearchCV with a RandomForestClassifer, your param_grid, and what you are optimizing for.  Choose any of the five optimization strategies; accuracy, precision, recall, f1, or roc_auc.\n",
    "random_forest =  GridSearchCV(RandomForestClassifier(),param_grid=params_rf, scoring='accuracy', cv=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit your GridSearchCV with your training data. \n",
    "random_forest.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the parameters of your best model. \n",
    "# Print the best parameters it found\n",
    "print(random_forest.best_estimator_) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate your best model using accuracy, precision, recall, f1 scores, and auc scores. \n",
    "\n",
    "# This command gives you tree that has the highest f1-score. \n",
    "model = random_forest.best_estimator_\n",
    "\n",
    "\n",
    "# Now lets evaluate our model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Accuracy Score: %f\" % accuracy_rf)\n",
    "\n",
    "precision_rf = precision_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Precision Score: %f\" % precision_rf)\n",
    "\n",
    "recall_rf = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Recall Score: %f\" % recall_rf)\n",
    "\n",
    "f1_rf = f1_score(y_true=y_test, y_pred=y_pred)\n",
    "print('F1 Score: %f' % f1_rf)\n",
    "\n",
    "# Calculate predicted probabilities, keep only probability for when class = 1\n",
    "y_pred_proba_rf = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "auc_rf = roc_auc_score(y_true=y_test, y_score=y_pred_proba_rf)\n",
    "print('AUC Score: %f' % auc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Which feature was your most important feature?\n",
    "# Now lets look at our feature importances\n",
    "feature_imp = pd.Series(model.feature_importances_,index=feature).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a random forest using the ny-vs-sf-housing.csv data. \n",
    "* Your target variable, aka the column you are trying to predict, aka your `y` variable is `in_sf`. \n",
    "* Can you get an accuracy above %88.8889?\n",
    "* What was your most important feature?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_sf</th>\n",
       "      <th>beds</th>\n",
       "      <th>bath</th>\n",
       "      <th>price</th>\n",
       "      <th>year_built</th>\n",
       "      <th>sqft</th>\n",
       "      <th>price_per_sqft</th>\n",
       "      <th>elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999000</td>\n",
       "      <td>1960</td>\n",
       "      <td>1000</td>\n",
       "      <td>999</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2750000</td>\n",
       "      <td>2006</td>\n",
       "      <td>1418</td>\n",
       "      <td>1939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1350000</td>\n",
       "      <td>1900</td>\n",
       "      <td>2150</td>\n",
       "      <td>628</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>629000</td>\n",
       "      <td>1903</td>\n",
       "      <td>500</td>\n",
       "      <td>1258</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>439000</td>\n",
       "      <td>1930</td>\n",
       "      <td>500</td>\n",
       "      <td>878</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   in_sf  beds  bath    price  year_built  sqft  price_per_sqft  elevation\n",
       "0      0   2.0   1.0   999000        1960  1000             999         10\n",
       "1      0   2.0   2.0  2750000        2006  1418            1939          0\n",
       "2      0   2.0   2.0  1350000        1900  2150             628          9\n",
       "3      0   1.0   1.0   629000        1903   500            1258          9\n",
       "4      0   0.0   1.0   439000        1930   500             878         10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/ny-vs-sf-houses.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of our Training data: 393 \n",
      "Length of our Testing data: 99\n"
     ]
    }
   ],
   "source": [
    "# BUILD, TRAIN, AND EVAULATE A RANDOM FOREST MODEL BELOW. \n",
    "house_features = ['year_built','sqft','price_per_sqft','elevation']\n",
    "X = df[house_features]\n",
    "y = df['in_sf']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,test_size=0.20,random_state=45)\n",
    "print('Lenght of our Training data:', X_train.shape[0], '\\nLength of our Testing data:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_rf_house = {\n",
    "    #'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [None, 10, 20, 30], \n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = GridSearchCV(RandomForestClassifier(),param_grid=params_rf_house)\n",
    "random_forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_forest.best_estimator_)\n",
    "model = random_forest.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets evaluate our model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy_rf_house = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score: %f\" % accuracy_rf_house)\n",
    "\n",
    "precision_rf_house = precision_score(y_test, y_pred)\n",
    "print(\"Precision Score: %f\" % precision_rf_house)\n",
    "\n",
    "recall_rf_house = recall_score(y_test, y_pred)\n",
    "print(\"Recall Score: %f\" % recall_rf_house)\n",
    "\n",
    "f1_rf_house = f1_score(y_test, y_pred)\n",
    "print('F1 Score: %f' % f1_rf_house)\n",
    "\n",
    "# Calculate predicted probabilities, keep only probability for when class = 1\n",
    "y_pred_proba_rf_house = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "auc_rf_house = roc_auc_score(y_true=y_test, y_score=y_pred_proba_rf_house)\n",
    "print('AUC Score: %f' % auc_rf_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome difficult extra credit below:\n",
    "Build a classifier using the adult_income.csv data.  \n",
    "* The target variable is 'class'\n",
    "* Start with just using these features `selected_features = ['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week']`\n",
    "* You have to include the pos_label in your precision, recall, and f1 scores. It just tells the classifier which one is the posotive label.  I provided the proper way below.\n",
    "\n",
    "* See if you can get above 50% f1 score.  \n",
    "* See some [super tricks and tips here](https://www.kaggle.com/code/jieyima/income-classification-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult_income.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
